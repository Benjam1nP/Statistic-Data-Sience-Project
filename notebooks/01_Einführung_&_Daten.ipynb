{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b05b811",
   "metadata": {},
   "source": [
    "# 01 Einführung & Daten\n",
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae38e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029d527",
   "metadata": {},
   "source": [
    "## Infos zum Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77133541",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/2023_Yellow_Taxi_Trip_Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chunksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1_000_000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m reader \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/2023_Yellow_Taxi_Trip_Data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunksize\u001b[38;5;241m=\u001b[39mchunksize)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Den ersten Chunk holen\u001b[39;00m\n\u001b[1;32m      5\u001b[0m first_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(reader)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/2023_Yellow_Taxi_Trip_Data.csv'"
     ]
    }
   ],
   "source": [
    "chunksize = 1_000_000\n",
    "reader = pd.read_csv(\"../data/2023_Yellow_Taxi_Trip_Data.csv\", chunksize=chunksize)\n",
    "\n",
    "# Den ersten Chunk holen\n",
    "first_chunk = next(reader)\n",
    "\n",
    "# Info anzeigen\n",
    "print(first_chunk.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60db4211",
   "metadata": {},
   "source": [
    "Wie wir sehen können ist der Datensatz recht gross und braucht viel Speicher, weshalb es sich nur in chunks bearbeiten lässt. \n",
    "\n",
    "Im Folgenden wird es vorallem darum gehen den Datensatz zu optimieren und wichtige Kennzahlen zu berechnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a90e35",
   "metadata": {},
   "source": [
    "## Beschreiben des Datensatzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             VendorID  passenger_count   trip_distance      RatecodeID  \\\n",
      "count  1000000.000000   1000000.000000  1000000.000000  1000000.000000   \n",
      "mean         1.732321         1.364828        3.372839        1.435825   \n",
      "std          0.442750         0.896679       24.952718        6.013780   \n",
      "min          1.000000         0.000000        0.000000        1.000000   \n",
      "25%          1.000000         1.000000        1.060000        1.000000   \n",
      "50%          2.000000         1.000000        1.760000        1.000000   \n",
      "75%          2.000000         1.000000        3.250000        1.000000   \n",
      "max          2.000000         9.000000     9683.760000       99.000000   \n",
      "\n",
      "         PULocationID    DOLocationID    payment_type     fare_amount  \\\n",
      "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
      "mean       166.420072      164.483686        1.215696       18.193923   \n",
      "std         64.097900       69.799382        0.496670       17.590646   \n",
      "min          1.000000        1.000000        1.000000     -600.000000   \n",
      "25%        132.000000      114.000000        1.000000        8.600000   \n",
      "50%        162.000000      162.000000        1.000000       12.800000   \n",
      "75%        234.000000      234.000000        1.000000       19.800000   \n",
      "max        265.000000      265.000000        4.000000      600.000000   \n",
      "\n",
      "                extra         mta_tax      tip_amount    tolls_amount  \\\n",
      "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
      "mean         1.546080        0.488131        3.397447        0.522641   \n",
      "std          1.782976        0.099536        3.830573        2.036974   \n",
      "min         -7.500000       -0.500000      -96.220000      -30.000000   \n",
      "25%          0.000000        0.500000        1.000000        0.000000   \n",
      "50%          1.000000        0.500000        2.790000        0.000000   \n",
      "75%          2.500000        0.500000        4.200000        0.000000   \n",
      "max         11.250000        4.000000      222.210000      196.990000   \n",
      "\n",
      "       improvement_surcharge    total_amount  congestion_surcharge  \\\n",
      "count          1000000.00000  1000000.000000        1000000.000000   \n",
      "mean                 0.98207       26.879859              2.286892   \n",
      "std                  0.18403       22.062870              0.754498   \n",
      "min                 -1.00000     -603.500000             -2.500000   \n",
      "25%                  1.00000       15.400000              2.500000   \n",
      "50%                  1.00000       20.100000              2.500000   \n",
      "75%                  1.00000       28.500000              2.500000   \n",
      "max                  1.00000      607.550000              2.500000   \n",
      "\n",
      "          airport_fee  \n",
      "count  1000000.000000  \n",
      "mean         0.103332  \n",
      "std          0.349554  \n",
      "min         -1.250000  \n",
      "25%          0.000000  \n",
      "50%          0.000000  \n",
      "75%          0.000000  \n",
      "max          1.250000  \n"
     ]
    }
   ],
   "source": [
    "second_chunk = next(reader)\n",
    "\n",
    "print(second_chunk.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca840a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der Datensatz enthält folgende Informationen\n",
      "Spaltennamen:\n",
      " ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "\n",
      "Anzahl Header:  19\n",
      "\n",
      "Die Datei hat ca. 38,310,226 Zeilen.\n"
     ]
    }
   ],
   "source": [
    "df_header = pd.read_csv(\"../data/2023_Yellow_Taxi_Trip_Data.csv\", nrows=0)\n",
    "print(\"Der Datensatz enthält folgende Informationen\")\n",
    "original_header = df_header.columns.tolist()\n",
    "print(\"Spaltennamen:\\n\", original_header)\n",
    "print(\"\\nAnzahl Header: \", len(original_header))\n",
    "\n",
    "with open(\"../data/2023_Yellow_Taxi_Trip_Data.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    total_lines = sum(1 for _ in f) - 1\n",
    "\n",
    "print(f\"\\nDie Datei hat ca. {total_lines:,} Zeilen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f362da",
   "metadata": {},
   "source": [
    "Um die Anzahl Zeilen zu erhalten konnte ich keine pandas einsetzen, da jedesmal das kernel kaputt gegangen ist. \n",
    "Unser Datensatz hat also um die 38 Millionen Zeilen/Taxifahrten im Jahr 2023 und zu einer Fahrt sind viel Daten vorhanden, wie zum Beispiel Preis, Uhrzeit und Distanz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e334d7",
   "metadata": {},
   "source": [
    "## Mögliche Datenverbesserung\n",
    "\n",
    "Dank der Info- und Describe-Funktionen können wir uns nun den gesamten Datensatz ansehen und erkennen, welche Spalten für unsere Analyse nicht nützlich sind. Diese Spalten werden gelöscht, um Arbeitsspeicher (RAM) zu sparen und die Datengröße zu reduzieren.\n",
    "\n",
    "- VendorID: Hat nur die Werte 1, 2 oder 6. Da wir diese Information nicht mit etwas Nützlichem verknüpfen können, kann die Spalte\n",
    "            gelöscht werden.\n",
    "\n",
    "- RatecodeID: Gleiches wie zuvor – die Werte sind meist 1 oder 99, in den meisten Fällen jedoch 1. Daher ist sie nicht hilfreich.\n",
    "\n",
    "- store_and_fwd_flag: Immer derselbe Wert und die Bedeutung ist unklar. Diese Spalte kann ebenfalls entfernt werden.\n",
    "\n",
    "- mta_tax: Hat immer denselben Wert (0.5) – es gibt zwar einige fehlerhafte Einträge, aber grundsätzlich keine Variation, daher kann\n",
    "           sie gelöscht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58591511",
   "metadata": {},
   "source": [
    "## Überprüfen der Spaltenlöschung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d927d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datensatz nach Löschung:\n",
      "\n",
      "Datensatz hat jetzt 15 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"Datensatz nach Löschung:\\n\")\n",
    "df = pd.read_csv(\"../data/Taxi_Data_improved.csv\", nrows=0)\n",
    "print(f\"Datensatz hat jetzt {df.shape[1]} Spalten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a6534",
   "metadata": {},
   "source": [
    "## Erstellen eines repräsentativen Datensatzes\n",
    "\n",
    "Unser Datensatz hat aktuell mehr als 38 Millionen Taxifahrten, es ist unmöglich (auch mit pandas) diesen in einem Stück einzulesen und zu analysieren, desshalb haben wir uns entschieden einen kleineren repräsentativen Datensatz zu erstellen. Zuerst wird das original chunkweise geshuffelt und anschliessen aus jedem gemischten chunk eine Prozentsatz an Daten random herausgenommen und in einem neuen Datensatz gespeichert. Der neue Datensatz sollte 1 Millionen Taxifahrten beinhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d65350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten des originalen Datensatzes: 38,310,226\n",
      "Daten des neuen Datensatzes: 999,997\n"
     ]
    }
   ],
   "source": [
    "print(f\"Daten des originalen Datensatzes: {total_lines:,}\")\n",
    "\n",
    "with open(\"../data/Taxi_sample_1M.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    new_total_lines = sum(1 for _ in f) - 1\n",
    "print(f\"Daten des neuen Datensatzes: {new_total_lines:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdb707",
   "metadata": {},
   "source": [
    "## Wechsel zu Datetime\n",
    "\n",
    "Aktuell liegt die Abfahrts und Ankunfts Zeit in der US-Datums form vor. Um die Zeiten für Rechnungen benutzen zu können, müssen wir die Zeitangaben in Datetime vorleigen haben. deshalb wurde das pythonscript change_to_datetime erstellt welches diese Angaben umwandelt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e55024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeitangabe vor Änderung:\n",
      "\n",
      "     tpep_pickup_datetime   tpep_dropoff_datetime\n",
      "0  01/01/2023 12:32:10 AM  01/01/2023 12:40:36 AM\n",
      "1  01/01/2023 12:55:08 AM  01/01/2023 01:01:27 AM\n",
      "2  01/01/2023 12:25:04 AM  01/01/2023 12:37:49 AM\n",
      "\n",
      "Zeitangabe nach Änderung:\n",
      "\n",
      "  tpep_pickup_datetime tpep_dropoff_datetime\n",
      "0  2023-02-06 18:31:28   2023-02-06 18:41:28\n",
      "1  2023-01-13 12:22:41   2023-01-13 12:54:42\n",
      "2  2023-01-24 12:53:51   2023-01-24 13:06:09\n"
     ]
    }
   ],
   "source": [
    "example_1 = pd.read_csv(\"../data/2023_Yellow_Taxi_Trip_Data.csv\", nrows=3)\n",
    "print(\"Zeitangabe vor Änderung:\\n\")\n",
    "print(example_1.iloc[:, 1:3])\n",
    "\n",
    "example_2 = pd.read_csv(\"../data/Taxi_final_1M.csv\", nrows=3)\n",
    "print(\"\\nZeitangabe nach Änderung:\\n\")\n",
    "print(example_2.iloc[:, :2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130026a",
   "metadata": {},
   "source": [
    "## Generieren von neuen Spalten\n",
    "\n",
    "Neben dem Wechsel zu Datetime, wird mit dem gleichen Pythonscript die Taxifahrtdauer und die durchschnitts Geschwindigkeit berechnet und hinten als neue Spalten eingefühgt. Zudem wird die Spalte trip_distance in Kilometer umgerechnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22946129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neu eingefühgte Spalten:\n",
      "\n",
      "   trip_duration  average_speed\n",
      "0           10.0           17.0\n",
      "1           32.0            0.0\n",
      "2           12.3           11.2\n",
      "3            4.4           15.5\n",
      "4           11.4           15.5\n",
      "5            5.7           13.1\n"
     ]
    }
   ],
   "source": [
    "example_3 = pd.read_csv(\"../data/Taxi_final_1M.csv\", nrows=6)\n",
    "print(\"\\nNeu eingefühgte Spalten:\\n\")\n",
    "print(example_3.iloc[:,15:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
